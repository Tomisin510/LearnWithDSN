{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODJOuBD636UWYA+IIuQ1Tc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomisin510/LearnWithDSN/blob/main/Week_4/Week_4_Applied_Learning_Assignments_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision Take-Home Assignment\n",
        "\n",
        "## **Name:** Tomisin Obijole\n",
        "## **Date:** 24 February 2026"
      ],
      "metadata": {
        "id": "J5Asz6ncG-Xw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applied Learning Assignments 3"
      ],
      "metadata": {
        "id": "lVR2bA_mHBk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 One-Hot Encoding\n",
        "Define a vocabulary of at least 5 unique words and generate one‑hot encoded vectors."
      ],
      "metadata": {
        "id": "dEigcFPiHJGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Vocabulary\n",
        "vocab = ['cat', 'dog', 'bird', 'fish', 'hamster']\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "def one_hot(word):\n",
        "    vec = np.zeros(len(vocab))\n",
        "    vec[word_to_index[word]] = 1\n",
        "    return vec\n",
        "\n",
        "# Test\n",
        "for word in vocab:\n",
        "    print(f\"{word}: {one_hot(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztqFszaGHUJb",
        "outputId": "56894660-5a59-4906-c5c7-2281008f9077"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: [1. 0. 0. 0. 0.]\n",
            "dog: [0. 1. 0. 0. 0.]\n",
            "bird: [0. 0. 1. 0. 0.]\n",
            "fish: [0. 0. 0. 1. 0.]\n",
            "hamster: [0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Bag of Words & TF‑IDF\n",
        "\n",
        "Use the sentences:\n",
        "\"The quick brown fox jumps over the lazy dog.\"\n",
        "\"The dog sleeps in the kernel\"\n",
        "Generate BoW and TF‑IDF representations."
      ],
      "metadata": {
        "id": "-KR1Xoq_HZQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog sleeps in the kernel\"\n",
        "]\n",
        "\n",
        "# Bag of Words\n",
        "count_vec = CountVectorizer()\n",
        "bow = count_vec.fit_transform(sentences)\n",
        "print(\"BoW feature names:\", count_vec.get_feature_names_out())\n",
        "print(\"BoW matrix:\\n\", bow.toarray())\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "tfidf = tfidf_vec.fit_transform(sentences)\n",
        "print(\"\\nTF-IDF feature names:\", tfidf_vec.get_feature_names_out())\n",
        "print(\"TF-IDF matrix:\\n\", tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhAsRh-GHBP5",
        "outputId": "8a45e658-cb8f-48c6-9f1d-7352e2240364"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW feature names: ['brown' 'dog' 'fox' 'in' 'jumps' 'kernel' 'lazy' 'over' 'quick' 'sleeps'\n",
            " 'the']\n",
            "BoW matrix:\n",
            " [[1 1 1 0 1 0 1 1 1 0 2]\n",
            " [0 1 0 1 0 1 0 0 0 1 2]]\n",
            "\n",
            "TF-IDF feature names: ['brown' 'dog' 'fox' 'in' 'jumps' 'kernel' 'lazy' 'over' 'quick' 'sleeps'\n",
            " 'the']\n",
            "TF-IDF matrix:\n",
            " [[0.342369   0.24359836 0.342369   0.         0.342369   0.\n",
            "  0.342369   0.342369   0.342369   0.         0.48719673]\n",
            " [0.         0.30253071 0.         0.42519636 0.         0.42519636\n",
            "  0.         0.         0.         0.42519636 0.60506143]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Word2Vec with Gensim\n",
        "\n",
        "Create a small dataset of at least 3 sentences about animals. Train a Word2Vec model and retrieve the embedding for \"dog\"."
      ],
      "metadata": {
        "id": "gYbp07F_Hk_x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU4p9yRkGfZB",
        "outputId": "b87226ae-f65c-4278-bc92-7c4c9ba5fd44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Training sentences: [['The', 'cat', 'meows'], ['The', 'dog', 'barks'], ['The', 'bird', 'sings']]\n",
            "\n",
            "Embedding for 'dog':\n",
            "[ 0.00018913  0.00615464 -0.01362529 -0.00275093  0.01533716  0.01469282\n",
            " -0.00734659  0.0052854  -0.01663426  0.01241097 -0.00927464 -0.00632821\n",
            "  0.01862271  0.00174677  0.01498141 -0.01214813  0.01032101  0.01984565\n",
            " -0.01691478 -0.01027138 -0.01412967 -0.0097253  -0.00755713 -0.0170724\n",
            "  0.01591121 -0.00968788  0.01684723  0.01052514 -0.01310005  0.00791574\n",
            "  0.0109403  -0.01485307 -0.01481144 -0.00495046 -0.01725145 -0.00316314\n",
            " -0.00080687  0.00659937  0.00288376 -0.00176284 -0.01118812  0.00346073\n",
            " -0.00179474  0.01358738  0.00794718  0.00905894  0.00286861 -0.00539971\n",
            " -0.00873363 -0.00206415]\n",
            "Shape: (50,)\n",
            "\n",
            "Words most similar to 'dog':\n",
            "  bird: 0.1656\n",
            "  sings: 0.1249\n",
            "  meows: 0.1023\n"
          ]
        }
      ],
      "source": [
        "# First, install gensim if not already installed\n",
        "!pip install gensim\n",
        "\n",
        "# Now import and run the Word2Vec code\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress any deprecation warnings\n",
        "\n",
        "# Dataset - properly tokenized sentences\n",
        "sentences = [\n",
        "    \"The cat meows\".split(),\n",
        "    \"The dog barks\".split(),\n",
        "    \"The bird sings\".split()\n",
        "]\n",
        "\n",
        "print(\"Training sentences:\", sentences)\n",
        "\n",
        "# Train Word2Vec model\n",
        "# vector_size: dimensionality of the word vectors\n",
        "# window: maximum distance between current and predicted word\n",
        "# min_count: ignores words with total frequency lower than this\n",
        "# workers: number of threads to use\n",
        "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, workers=4)\n",
        "\n",
        "# Get embedding for \"dog\"\n",
        "if 'dog' in model.wv:\n",
        "    print(\"\\nEmbedding for 'dog':\")\n",
        "    print(model.wv['dog'])\n",
        "    print(f\"Shape: {model.wv['dog'].shape}\")\n",
        "else:\n",
        "    print(\"'dog' not in vocabulary\")\n",
        "\n",
        "# Find similar words (though with such a small dataset, results may be limited)\n",
        "if 'dog' in model.wv:\n",
        "    try:\n",
        "        similar = model.wv.most_similar('dog', topn=3)\n",
        "        print(\"\\nWords most similar to 'dog':\")\n",
        "        for word, score in similar:\n",
        "            print(f\"  {word}: {score:.4f}\")\n",
        "    except:\n",
        "        print(\"Could not find similar words (dataset too small)\")\n",
        "\n",
        "# Save and load the model (optional)\n",
        "# model.save(\"word2vec_animal.model\")\n",
        "# loaded_model = Word2Vec.load(\"word2vec_animal.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Pretrained GloVe with Gensim\n",
        "\n",
        "Load the GloVe model (glove-wiki-gigaword-50) and retrieve embedding for \"king\" and its 5 most similar words."
      ],
      "metadata": {
        "id": "CHTSGf-8IMmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the GloVe model (this may take a few minutes the first time)\n",
        "glove = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# Embedding for \"king\"\n",
        "if 'king' in glove:\n",
        "    print(\"Embedding for 'king':\", glove['king'])\n",
        "else:\n",
        "    print(\"'king' not in vocabulary\")\n",
        "\n",
        "# 5 most similar words to \"king\"\n",
        "if 'king' in glove:\n",
        "    similar = glove.most_similar('king', topn=5)\n",
        "    print(\"Most similar words to 'king':\", similar)\n",
        "else:\n",
        "    print(\"Cannot find 'king'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZHxmZJhISzE",
        "outputId": "64334693-1be6-415a-b679-ff574eca5653"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Embedding for 'king': [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
            "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
            "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
            " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
            " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
            "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
            " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
            " -0.51042 ]\n",
            "Most similar words to 'king': [('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721)]\n"
          ]
        }
      ]
    }
  ]
}